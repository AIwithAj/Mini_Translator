{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lenovo\\\\Desktop\\\\Mini_Translator\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lenovo\\\\Desktop\\\\Mini_Translator'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_1: str\n",
    "    tokenizer_2:str\n",
    "    lang1:str\n",
    "    lang2:str\n",
    "    sos_token: str\n",
    "    eos_token: str\n",
    "    max_length : int\n",
    "    lower: bool\n",
    "    data_loader:Path\n",
    "    batch_size: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Mini_Translator.constants import *\n",
    "from src.Mini_Translator.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        params=self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_1=config.tokenizer_1,\n",
    "            tokenizer_2=config.tokenizer_2,\n",
    "            lang1=params.lang1,\n",
    "            lang2=params.lang2,\n",
    "            sos_token=params.sos_token,\n",
    "            eos_token=params.eos_token,\n",
    "            max_length=params.max_length,\n",
    "            lower=params.lower,\n",
    "            data_loader=config.data_loader,\n",
    "            batch_size=params.batch_size\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.Mini_Translator.logging import logger\n",
    "import spacy\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from src.Mini_Translator.logging import logger  # Make sure to import the logger\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig, config_filepath=CONFIG_FILE_PATH):\n",
    "        self.config = config\n",
    "        # os.system(f\"python -m spacy download {config.tokenizer_1}\")\n",
    "        # os.system(f\"python -m spacy download {config.tokenizer_2}\")\n",
    "        self.config2 = read_yaml(config_filepath)\n",
    "\n",
    "    def tokenize_example(self, example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "        en_tokens = [token.text for token in en_nlp.tokenizer(example[self.config.lang1])][:max_length]\n",
    "        de_tokens = [token.text for token in de_nlp.tokenizer(example[self.config.lang2])][:max_length]\n",
    "        if lower:\n",
    "            en_tokens = [token.lower() for token in en_tokens]\n",
    "            de_tokens = [token.lower() for token in de_tokens]\n",
    "        en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "        de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "        return {f\"{self.config.lang1}_tokens\": en_tokens, f\"{self.config.lang2}_tokens\": de_tokens}\n",
    "    \n",
    "    def numericalize_example(self, example, en_vocab, de_vocab):\n",
    "        en_ids = en_vocab.lookup_indices(example[f\"{self.config.lang1}_tokens\"])\n",
    "        de_ids = de_vocab.lookup_indices(example[f\"{self.config.lang2}_tokens\"])\n",
    "        return {f\"{self.config.lang1}_ids\": en_ids, f\"{self.config.lang2}_ids\": de_ids}\n",
    "    \n",
    "    def get_collate_fn(self, pad_index):\n",
    "        def collate_fn(batch):\n",
    "            batch_en_ids = [example[f\"{self.config.lang1}_ids\"] for example in batch]\n",
    "            batch_de_ids = [example[f\"{self.config.lang2}_ids\"] for example in batch]\n",
    "            batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "            batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "            return {f\"{self.config.lang1}_ids\": batch_en_ids, f\"{self.config.lang2}_ids\": batch_de_ids}\n",
    "        return collate_fn\n",
    "    \n",
    "    def get_data_loader(self, dataset, batch_size, pad_index, shuffle=False):\n",
    "        collate_fn = self.get_collate_fn(pad_index)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    def initiate_tokenization(self):\n",
    "        en_nlp = spacy.load(self.config.tokenizer_1)\n",
    "        de_nlp = spacy.load(self.config.tokenizer_2)\n",
    "        ingestion_config = self.config2.data_ingestion\n",
    "\n",
    "        with open(ingestion_config.data_files.train, 'r') as json_file:\n",
    "            train_data = json.load(json_file)\n",
    "\n",
    "        with open(ingestion_config.data_files.validation, 'r') as json_file:\n",
    "            valid_data = json.load(json_file)\n",
    "\n",
    "        with open(ingestion_config.data_files.test, 'r') as json_file:\n",
    "            test_data = json.load(json_file)\n",
    "\n",
    "        # Convert the loaded lists of dictionaries to datasets.Dataset objects\n",
    "        train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "        valid_dataset = Dataset.from_pandas(pd.DataFrame(valid_data))\n",
    "        test_dataset = Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "\n",
    "        # Tokenize using map\n",
    "        fn_kwargs = {\n",
    "            \"en_nlp\": en_nlp,\n",
    "            \"de_nlp\": de_nlp,\n",
    "            \"max_length\": self.config.max_length,\n",
    "            \"lower\": self.config.lower,\n",
    "            \"sos_token\": self.config.sos_token,\n",
    "            \"eos_token\": self.config.eos_token\n",
    "        }\n",
    "\n",
    "        def tokenize_wrapper(example):\n",
    "            return self.tokenize_example(example, **fn_kwargs)\n",
    "\n",
    "        train_dataset = train_dataset.map(tokenize_wrapper)\n",
    "        valid_dataset = valid_dataset.map(tokenize_wrapper)\n",
    "        test_dataset = test_dataset.map(tokenize_wrapper)\n",
    "\n",
    "        logger.info(f\"After tokenization, type of train_data: {type(train_dataset)}\")\n",
    "\n",
    "        # Building vocabulary\n",
    "        min_freq = 2\n",
    "        unk_token = \"<unk>\"\n",
    "        pad_token = \"<pad>\"\n",
    "        special_tokens = [unk_token, pad_token, self.config.sos_token, self.config.eos_token]\n",
    "\n",
    "        en_vocab = build_vocab_from_iterator(\n",
    "            (x[f\"{self.config.lang1}_tokens\"] for x in train_dataset),\n",
    "            min_freq=min_freq,\n",
    "            specials=special_tokens,\n",
    "        )\n",
    "        de_vocab = build_vocab_from_iterator(\n",
    "            (x[f\"{self.config.lang2}_tokens\"] for x in train_dataset),\n",
    "            min_freq=min_freq,\n",
    "            specials=special_tokens,\n",
    "        )\n",
    "        torch.save(en_vocab, 'en_vocab.pth')\n",
    "        torch.save(de_vocab, 'de_vocab.pth')\n",
    "        #         torch.save(en_vocab, os.path.join(self.config.root_dir,'vocab/en_vocab.pth'))\n",
    "        # torch.save(de_vocab, os.path.join(self.config.root_dir,'vocab/de_vocab.pth'))\n",
    "\n",
    "        assert en_vocab[unk_token] == de_vocab[unk_token]\n",
    "        assert en_vocab[pad_token] == de_vocab[pad_token]\n",
    "        unk_index = en_vocab[unk_token]\n",
    "        pad_index = en_vocab[pad_token]\n",
    "\n",
    "        en_vocab.set_default_index(en_vocab[unk_token])\n",
    "        de_vocab.set_default_index(de_vocab[unk_token])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        logger.info(f\"English vocab size: {len(en_vocab)}\")\n",
    "        logger.info(f\"German vocab size: {len(de_vocab)}\")\n",
    "\n",
    "        with open(\"metadata.json\", 'w') as file:\n",
    "            json.dump({\n",
    "                'en_vocab': len(en_vocab),\n",
    "                'de_vocab': len(de_vocab),\n",
    "                'pad_index': pad_index,\n",
    "                'unk_index': unk_index,\n",
    "                \n",
    "            }, file, indent=4)\n",
    "\n",
    "        fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
    "\n",
    "        def numericalize_wrapper(example):\n",
    "            return self.numericalize_example(example, **fn_kwargs)\n",
    "\n",
    "        train_dataset = train_dataset.map(numericalize_wrapper)\n",
    "        valid_dataset = valid_dataset.map(numericalize_wrapper)\n",
    "        test_dataset = test_dataset.map(numericalize_wrapper)\n",
    "\n",
    "        data_type = \"torch\"\n",
    "        format_columns = [f\"{self.config.lang1}_ids\", f\"{self.config.lang2}_ids\"]\n",
    "        train_dataset = train_dataset.with_format(type=data_type, columns=format_columns, output_all_columns=True)\n",
    "        valid_dataset = valid_dataset.with_format(type=data_type, columns=format_columns, output_all_columns=True)\n",
    "        test_dataset = test_dataset.with_format(type=data_type, columns=format_columns, output_all_columns=True)\n",
    "\n",
    "        logger.info(type(train_dataset[0][f\"{self.config.lang2}_ids\"]))\n",
    "\n",
    "        root_dir = self.config.root_dir\n",
    "        train_dataset.save_to_disk(os.path.join(root_dir, \"train_dataset\"))\n",
    "        valid_dataset.save_to_disk(os.path.join(root_dir, \"valid_dataset\"))\n",
    "        test_dataset.save_to_disk(os.path.join(root_dir, \"test_dataset\"))\n",
    "\n",
    "        # Create dataloaders (batches)\n",
    "        batch_size = self.config.batch_size\n",
    "        train_data_loader = self.get_data_loader(train_dataset, batch_size, pad_index, shuffle=True)\n",
    "        valid_data_loader = self.get_data_loader(valid_dataset, batch_size, pad_index)\n",
    "        test_data_loader = self.get_data_loader(test_dataset, batch_size, pad_index)\n",
    "\n",
    "        logger.info(f\"Length of train data loader: {len(train_data_loader)}\")\n",
    "        logger.info(f\"Length of valid data loader: {len(valid_data_loader)}\")\n",
    "        logger.info(f\"Length of test data loader: {len(test_data_loader)}\")\n",
    "\n",
    "        # Saving dataloaders as list of batches\n",
    "        root_dir=self.config.root_dir\n",
    "        torch.save(list(train_data_loader), os.path.join(root_dir, \"train_data_loader.pth\"))  # Changed line\n",
    "        torch.save(list(valid_data_loader), os.path.join(root_dir, \"valid_data_loader.pth\"))  # Changed line\n",
    "        torch.save(list(test_data_loader), os.path.join(root_dir, \"test_data_loader.pth\"))    # Changed line\n",
    "\n",
    "        logger.info(f\"Data loaders saved to {root_dir}\")\n",
    "        logger.info(\"Data transformation successfully completed\")\n",
    "\n",
    "# Example usage\n",
    "# config = DataTransformationConfig(...)\n",
    "# data_transformation = DataTransformation(config)\n",
    "# data_transformation.initiate_tokenization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 21:29:50,911: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-22 21:29:50,918: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-22 21:29:50,926: INFO: common: created directory at: artifacts]\n",
      "[2024-05-22 21:29:50,930: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-05-22 21:29:50,968: INFO: common: yaml file: config\\config.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29000/29000 [00:34<00:00, 836.21 examples/s] \n",
      "Map: 100%|██████████| 1014/1014 [00:01<00:00, 626.52 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1160.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 21:30:43,783: INFO: 1267986715: After tokenization, type of train_data: <class 'datasets.arrow_dataset.Dataset'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 21:31:02,277: INFO: 1267986715: English vocab size: 5893]\n",
      "[2024-05-22 21:31:02,285: INFO: 1267986715: German vocab size: 7853]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29000/29000 [00:10<00:00, 2771.17 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:00<00:00, 2415.96 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3080.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 21:31:13,986: INFO: 1267986715: <class 'torch.Tensor'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 29000/29000 [00:00<00:00, 187223.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1014/1014 [00:00<00:00, 40518.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 40223.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 21:31:14,244: INFO: 1267986715: Length of train data loader: 227]\n",
      "[2024-05-22 21:31:14,244: INFO: 1267986715: Length of valid data loader: 8]\n",
      "[2024-05-22 21:31:14,253: INFO: 1267986715: Length of test data loader: 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 21:31:29,288: INFO: 1267986715: Data loaders saved to artifacts/data_transformation]\n",
      "[2024-05-22 21:31:29,288: INFO: 1267986715: Data transformation successfully completed]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.initiate_tokenization()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
