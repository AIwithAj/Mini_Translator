{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lenovo\\\\Desktop\\\\Mini_Translator\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    root_dir: Path\n",
    "    encoder_embedding_dim: int\n",
    "    decoder_embedding_dim: int\n",
    "    hidden_dim: int\n",
    "    n_layers: int\n",
    "    encoder_dropout: float\n",
    "    decoder_dropout: float\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Mini_Translator.constants import *\n",
    "from src.Mini_Translator.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_config(self) -> ModelConfig:\n",
    "        config = self.config.base_Model\n",
    "        params=self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_config=ModelConfig(root_dir=config.root_dir,\n",
    "                                 encoder_embedding_dim =params.encoder_embedding_dim,\n",
    "                        decoder_embedding_dim = params.decoder_embedding_dim,\n",
    "                        hidden_dim = params.hidden_dim,\n",
    "                        n_layers= params.n_layers,\n",
    "                        encoder_dropout = params.encoder_dropout,\n",
    "                        decoder_dropout = params.decoder_dropout)\n",
    "        \n",
    "        return model_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,input_dim,embedding_dim,hidden_dim,n_layers,dropout):\n",
    "    super().__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.n_layers=n_layers\n",
    "    self.embedding=nn.Embedding(input_dim,embedding_dim)\n",
    "    self.lstm=nn.LSTM(embedding_dim,hidden_dim,n_layers,dropout=dropout)\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    #src=[src length,batch_size]\n",
    "    embedded=self.dropout(self.embedding(src))\n",
    "    #embedded=[src length ,batch size,embedding_dim]\n",
    "    outputs,(hidden,cell)=self.lstm(embedded)\n",
    "    #outputs=[src length ,batch size,hidden dim * n directions]\n",
    "\n",
    "    #hidden=[n layers * n directions ,batch size, hidden dim ]\n",
    "    #cell=[n layers * n directions,batch size, hidden dim  ]\n",
    "    return hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # context = [n layers, batch size, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # cell = [n layers, batch size, hidden dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size] #src sentence\n",
    "        # trg = [trg length, batch size] #target sentence\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        # input = [batch size]\n",
    "        # first we pass all sos 128\n",
    "        for t in range(1, trg_length):  # 1 to 34 not include 34\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            # cell = [n layers, batch size, hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.Mini_Translator.logging import logger\n",
    "\n",
    "class Base_Model:\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def initiate_prepare_base_model(self):\n",
    "        with open('metadata.json', 'r') as file:\n",
    "            f = json.load(file)\n",
    "            en_vocab = f['en_vocab']\n",
    "            de_vocab = f['de_vocab']\n",
    "\n",
    "        input_dim = de_vocab\n",
    "        output_dim = en_vocab\n",
    "\n",
    "        encoder_embedding_dim = self.config.encoder_embedding_dim\n",
    "        decoder_embedding_dim = self.config.decoder_embedding_dim\n",
    "        hidden_dim = self.config.hidden_dim\n",
    "        n_layers = self.config.n_layers\n",
    "        encoder_dropout = self.config.encoder_dropout\n",
    "        decoder_dropout = self.config.decoder_dropout\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        encoder = Encoder(input_dim, encoder_embedding_dim, hidden_dim, n_layers, encoder_dropout)\n",
    "        decoder = Decoder(output_dim, decoder_embedding_dim, hidden_dim, n_layers, decoder_dropout)\n",
    "        model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "        def init_weights(m):\n",
    "            for name, param in m.named_parameters():\n",
    "                nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        model.apply(init_weights)\n",
    "\n",
    "        def count_parameters(model):\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"The Model has {count_parameters(model):,} trainable parameters\")\n",
    "\n",
    "        root_dir = self.config.root_dir\n",
    "\n",
    "        # Save the complete model\n",
    "        model_path = os.path.join(root_dir, 'complete_model.pth')\n",
    "        torch.save(model, model_path)\n",
    "        logger.info(f\"Complete model saved to {model_path}\")\n",
    "\n",
    "        # Update metadata.json with device information\n",
    "        metadata_path = 'metadata.json'\n",
    "        with open(metadata_path, 'r') as file:\n",
    "            metadata = json.load(file)\n",
    "\n",
    "        metadata['device'] = str(device)\n",
    "\n",
    "        with open(metadata_path, 'w') as file:\n",
    "            json.dump(metadata, file, indent=4)\n",
    "        logger.info(f\"Device information saved to {metadata_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 22:11:33,849: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-22 22:11:33,858: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-22 22:11:33,858: INFO: common: created directory at: artifacts]\n",
      "[2024-05-22 22:11:33,858: INFO: common: created directory at: artifacts/base_model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 22:11:35,689: INFO: 3222732677: The Model has 13,898,501 trainable parameters]\n",
      "[2024-05-22 22:11:35,840: INFO: 3222732677: Complete model saved to artifacts/base_model\\complete_model.pth]\n",
      "[2024-05-22 22:11:35,855: INFO: 3222732677: Device information saved to metadata.json]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    get_model_config = config.get_model_config()\n",
    "    base_model = Base_Model(config=get_model_config)\n",
    "    base_model.initiate_prepare_base_model()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
