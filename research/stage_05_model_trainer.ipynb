{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class model_trainer_config:\n",
    "    root_dir: Path\n",
    "        \n",
    "    n_epochs: int\n",
    "    clip: float\n",
    "    teacher_forcing_ratio:  float\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Mini_Translator.constants import *\n",
    "from src.Mini_Translator.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_train_model_config(self) -> model_trainer_config:\n",
    "        config = self.config.model_trainer\n",
    "        params=self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        trainer_config=model_trainer_config(root_dir=config.root_dir,n_epochs=params.n_epochs,clip=params.clip,\n",
    "                                          teacher_forcing_ratio=params.teacher_forcing_ratio)\n",
    "        \n",
    "        return trainer_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.Mini_Translator.logging import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelTrainer:\n",
    "    def __init__(self,config:model_trainer_config,config_filepath = CONFIG_FILE_PATH):\n",
    "        self.config=config\n",
    "        self.config2=config_filepath\n",
    "\n",
    "    def train_fn(self,model,data_loader,optimizer,criterion,clip,teacher_forcing_ratio, device):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        i=0\n",
    "        for  batch in data_loader:\n",
    "            if i<220:\n",
    "                i=i+1\n",
    "                continue\n",
    "            print(i)\n",
    "            i=i+1\n",
    "            src=batch[\"de_ids\"].to(device)\n",
    "            #src=[src length , batch size]\n",
    "            trg=batch[\"en_ids\"].to(device)\n",
    "            #trg=[trg length ,batch size]\n",
    "            optimizer.zero_grad()\n",
    "            output=model(src,trg,teacher_forcing_ratio)\n",
    "            #output=[trg length , batch size, trg vocab size]\n",
    "            output_dim=output.shape[-1]\n",
    "            output=output[1:].view(-1,output_dim)\n",
    "            #output=[(trg length -1) * batch size ,trg vocab size]\n",
    "            trg=trg[1:].view(-1)\n",
    "            loss=criterion(output,trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "            optimizer.step()\n",
    "            epoch_loss +=loss.item()\n",
    "        return epoch_loss / len(data_loader)\n",
    "    \n",
    "    def evaluate_fn(self,model,data_loader,criterion,device):\n",
    "        model.eval()\n",
    "        epoch_loss=0\n",
    "        i=4\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "\n",
    "                if i<4:\n",
    "                    i=i+1\n",
    "                    continue\n",
    "                print(i)\n",
    "                src=batch[\"de_ids\"].to(device)\n",
    "                trg=batch[\"en_ids\"].to(device)\n",
    "                output=model(src,trg,0)\n",
    "                output_dim=output.shape[-1]\n",
    "                output=output[1:].view(-1,output_dim)\n",
    "                trg=trg[1:].view(-1)\n",
    "                loss=criterion(output,trg)\n",
    "                epoch_loss+=loss.item()\n",
    "        return epoch_loss / len(data_loader)\n",
    "    \n",
    "\n",
    "    def initiate_model_trainer(self):\n",
    "        root_dir = \"artifacts/data_transformation\"\n",
    "        train_data_loader_path = os.path.join(root_dir, \"train_data_loader.pth\")\n",
    "        valid_data_loader_path = os.path.join(root_dir, \"valid_data_loader.pth\")\n",
    "\n",
    "        # Load the DataLoader objects\n",
    "        train_data_loader = torch.load(train_data_loader_path)\n",
    "        valid_data_loader = torch.load(valid_data_loader_path)\n",
    "\n",
    "        model_path = os.path.join(\"artifacts/base_model\", 'complete_model.pth')\n",
    "\n",
    "        model = torch.load(model_path)\n",
    "\n",
    "        optimizer=optim.Adam(model.parameters())\n",
    "\n",
    "        with open(\"metadata.json\",'r') as file:\n",
    "            f=json.load(file)\n",
    "            pad_index=f[\"pad_index\"]\n",
    "            device=f[\"device\"]\n",
    "        criterion=nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "        n_epochs=1\n",
    "        clip=self.config.clip\n",
    "        teacher_forcing_ratio= self.config.teacher_forcing_ratio\n",
    "        best_valid_loss=float(\"inf\")\n",
    "        for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "            train_loss=self.train_fn(model,train_data_loader,optimizer,criterion,clip,teacher_forcing_ratio,device)\n",
    "            valid_loss=self.evaluate_fn(model,valid_data_loader,criterion,device)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss=valid_loss\n",
    "                torch.save(model.state_dict(),os.path.join(self.config.root_dir,\"tut1-model.pt\"))\n",
    "        # Calculate perplexities\n",
    "        train_ppl = np.exp(train_loss)\n",
    "        valid_ppl = np.exp(valid_loss)\n",
    "\n",
    "        # Print the losses and perplexities\n",
    "        print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {train_ppl:7.3f}\")\n",
    "        print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {valid_ppl:7.3f}\")\n",
    "\n",
    "        # Create a dictionary to hold the information\n",
    "        results = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_ppl\": train_ppl,\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"valid_ppl\": valid_ppl\n",
    "        }\n",
    "\n",
    "        # Specify the path to the JSON file\n",
    "        json_path = os.path.join(self.config.root_dir,\"results.json\")\n",
    "\n",
    "        # Save the results to a JSON file\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "        logger.info(f\"Results saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,input_dim,embedding_dim,hidden_dim,n_layers,dropout):\n",
    "    super().__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.n_layers=n_layers\n",
    "    self.embedding=nn.Embedding(input_dim,embedding_dim)\n",
    "    self.lstm=nn.LSTM(embedding_dim,hidden_dim,n_layers,dropout=dropout)\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    #src=[src length,batch_size]\n",
    "    embedded=self.dropout(self.embedding(src))\n",
    "    #embedded=[src length ,batch size,embedding_dim]\n",
    "    outputs,(hidden,cell)=self.lstm(embedded)\n",
    "    #outputs=[src length ,batch size,hidden dim * n directions]\n",
    "\n",
    "    #hidden=[n layers * n directions ,batch size, hidden dim ]\n",
    "    #cell=[n layers * n directions,batch size, hidden dim  ]\n",
    "    return hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # context = [n layers, batch size, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # cell = [n layers, batch size, hidden dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size] #src sentence\n",
    "        # trg = [trg length, batch size] #target sentence\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        # input = [batch size]\n",
    "        # first we pass all sos 128\n",
    "        for t in range(1, trg_length):  # 1 to 34 not include 34\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            # cell = [n layers, batch size, hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-24 08:40:00,252: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-24 08:40:00,262: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-24 08:40:00,266: INFO: common: created directory at: artifacts]\n",
      "[2024-05-24 08:40:00,269: INFO: common: created directory at: artifacts/trained_model]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:44<00:00, 104.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   0.233 | Train PPL:   1.263\n",
      "\tValid Loss:   5.811 | Valid PPL: 334.055\n",
      "[2024-05-24 08:41:46,342: INFO: 2547751249: Results saved to artifacts/trained_model\\results.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    get_model_config = config.get_train_model_config()\n",
    "    model = modelTrainer(config=get_model_config)\n",
    "    model.initiate_model_trainer()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
