{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class model_trainer_config:\n",
    "    root_dir: Path\n",
    "        \n",
    "    n_epochs: int\n",
    "    clip: float\n",
    "    teacher_forcing_ratio:  float\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Mini_Translator.constants import *\n",
    "from src.Mini_Translator.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_train_model_config(self) -> model_trainer_config:\n",
    "        config = self.config.model_trainer\n",
    "        params=self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        trainer_config=model_trainer_config(root_dir=config.root_dir,n_epochs=params.n_epochs,clip=params.clip,\n",
    "                                          teacher_forcing_ratio=params.teacher_forcing_ratio)\n",
    "        \n",
    "        return trainer_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.Mini_Translator.logging import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelTrainer:\n",
    "    def __init__(self,config:model_trainer_config,config_filepath = CONFIG_FILE_PATH):\n",
    "        self.config=config\n",
    "        self.config2=config_filepath\n",
    "\n",
    "    def train_fn(self,model,data_loader,optimizer,criterion,clip,teacher_forcing_ratio, device):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src=batch[\"de_ids\"].to(device)\n",
    "            #src=[src length , batch size]\n",
    "            trg=batch[\"en_ids\"].to(device)\n",
    "            #trg=[trg length ,batch size]\n",
    "            optimizer.zero_grad()\n",
    "            output=model(src,trg,teacher_forcing_ratio)\n",
    "            #output=[trg length , batch size, trg vocab size]\n",
    "            output_dim=output.shape[-1]\n",
    "            output=output[1:].view(-1,output_dim)\n",
    "            #output=[(trg length -1) * batch size ,trg vocab size]\n",
    "            trg=trg[1:].view(-1)\n",
    "            loss=criterion(output,trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "            optimizer.step()\n",
    "            epoch_loss +=loss.item()\n",
    "        return epoch_loss / len(data_loader)\n",
    "    \n",
    "    def evaluate_fn(self,model,data_loader,criterion,device):\n",
    "        model.eval()\n",
    "        epoch_loss=0\n",
    "        with torch.no_grad():\n",
    "            for i,batch in enumerate(data_loader):\n",
    "                src=batch[\"de_ids\"].to(device)\n",
    "                trg=batch[\"en_ids\"].to(device)\n",
    "                output=model(src,trg,0)\n",
    "                output_dim=output.shape[-1]\n",
    "                output=output[1:].view(-1,output_dim)\n",
    "                trg=trg[1:].view(-1)\n",
    "                loss=criterion(output,trg)\n",
    "                epoch_loss+=loss.item()\n",
    "        return epoch_loss / len(data_loader)\n",
    "    \n",
    "\n",
    "    def initiate_model_trainer(self):\n",
    "        root_dir = \"artifacts/data_transformation\"\n",
    "        train_data_loader_path = os.path.join(root_dir, \"train_data_loader.pth\")\n",
    "        valid_data_loader_path = os.path.join(root_dir, \"valid_data_loader.pth\")\n",
    "\n",
    "        # Load the DataLoader objects\n",
    "        train_data_loader = torch.load(train_data_loader_path)\n",
    "        valid_data_loader = torch.load(valid_data_loader_path)\n",
    "\n",
    "        model_path = os.path.join(\"artifacts/base_model\", 'complete_model.pth')\n",
    "\n",
    "        model = torch.load(model_path)\n",
    "\n",
    "        optimizer=optim.Adam(model.parameters())\n",
    "\n",
    "        with open(\"metadata.json\",'r') as file:\n",
    "            f=json.load(file)\n",
    "            pad_index=f[\"pad_index\"]\n",
    "            device=f[\"device\"]\n",
    "        criterion=nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "        n_epochs=2\n",
    "        clip=self.config.clip\n",
    "        teacher_forcing_ratio= self.config.teacher_forcing_ratio\n",
    "        best_valid_loss=float(\"inf\")\n",
    "        for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "            train_loss=self.train_fn(model,train_data_loader,optimizer,criterion,clip,teacher_forcing_ratio,device)\n",
    "            valid_loss=self.evaluate_fn(model,valid_data_loader,criterion,device)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss=valid_loss\n",
    "                torch.save(model.state_dict(),os.path.join(self.config.root_dir,\"tut1-model.pt\"))\n",
    "        # Calculate perplexities\n",
    "        train_ppl = np.exp(train_loss)\n",
    "        valid_ppl = np.exp(valid_loss)\n",
    "\n",
    "        # Print the losses and perplexities\n",
    "        print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {train_ppl:7.3f}\")\n",
    "        print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {valid_ppl:7.3f}\")\n",
    "\n",
    "        # Create a dictionary to hold the information\n",
    "        results = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_ppl\": train_ppl,\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"valid_ppl\": valid_ppl\n",
    "        }\n",
    "\n",
    "        # Specify the path to the JSON file\n",
    "        json_path = os.path.join(self.config.root_dir,\"results.json\")\n",
    "\n",
    "        # Save the results to a JSON file\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "        logger.info(f\"Results saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,input_dim,embedding_dim,hidden_dim,n_layers,dropout):\n",
    "    super().__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.n_layers=n_layers\n",
    "    self.embedding=nn.Embedding(input_dim,embedding_dim)\n",
    "    self.lstm=nn.LSTM(embedding_dim,hidden_dim,n_layers,dropout=dropout)\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, src):\n",
    "    #src=[src length,batch_size]\n",
    "    embedded=self.dropout(self.embedding(src))\n",
    "    #embedded=[src length ,batch size,embedding_dim]\n",
    "    outputs,(hidden,cell)=self.lstm(embedded)\n",
    "    #outputs=[src length ,batch size,hidden dim * n directions]\n",
    "\n",
    "    #hidden=[n layers * n directions ,batch size, hidden dim ]\n",
    "    #cell=[n layers * n directions,batch size, hidden dim  ]\n",
    "    return hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # context = [n layers, batch size, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # cell = [n layers, batch size, hidden dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size] #src sentence\n",
    "        # trg = [trg length, batch size] #target sentence\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        # input = [batch size]\n",
    "        # first we pass all sos 128\n",
    "        for t in range(1, trg_length):  # 1 to 34 not include 34\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            # cell = [n layers, batch size, hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-23 18:14:19,143: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-23 18:14:19,292: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-23 18:14:19,301: INFO: common: created directory at: artifacts]\n",
      "[2024-05-23 18:14:19,303: INFO: common: created directory at: artifacts/trained_model]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [02:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     get_model_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_train_model_config()\n\u001b[0;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m modelTrainer(config\u001b[38;5;241m=\u001b[39mget_model_config)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_model_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[6], line 70\u001b[0m, in \u001b[0;36mmodelTrainer.initiate_model_trainer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m best_valid_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):\n\u001b[1;32m---> 70\u001b[0m     train_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     valid_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_fn(model,valid_data_loader,criterion,device)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m, in \u001b[0;36mmodelTrainer.train_fn\u001b[1;34m(self, model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\u001b[0m\n\u001b[0;32m     20\u001b[0m trg\u001b[38;5;241m=\u001b[39mtrg[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m=\u001b[39mcriterion(output,trg)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(),clip)\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    get_model_config = config.get_train_model_config()\n",
    "    model = modelTrainer(config=get_model_config)\n",
    "    model.initiate_model_trainer()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
